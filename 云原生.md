## Docker
### cgroup和namespace
**namespace 是用来做资源隔离, cgroup 是用来做资源限制。**
#### cgroup
CGroups用来限定一个进程的资源使用，由Linux 内核支持，可以限制和隔离Linux进程组 (process groups) 所使用的物理资源 ，比如cpu、内存、磁盘。

#### namespace
namespace用来限定一个进程可见的资源，隔离PID(进程ID)、IPC、Network等系统资源。
在每一个namespace内部，每一个用户都拥有一个属于自己的init进程，pid = 1。  
父容器有两个子容器，父容器的命名空间里有两个进程，id分别为3和4, 映射到两个子命名空间后，分别成为其init进程，这样命名空间A和B的用户都认为自己独占整台服务器。
![image](https://user-images.githubusercontent.com/35059921/179401879-145dd2ce-c5bf-4ae8-8803-25fe5d62d8fb.png)  
查看namespace信息：
```
ll /proc/$$/ns
```

### 容器和镜像
![05829b9f34706649fe2c5e683c76b3f](https://user-images.githubusercontent.com/35059921/179188557-5f2bc422-840b-472d-81e9-99402836ebab.png)
#### 镜像
镜像（Image）就是一堆只读层（read-only layer）的统一视角。
#### 容器
容器（container）的定义和镜像（image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。
#### 运行态容器
一个运行态容器（running container）被定义为一个可读写的统一文件系统加上隔离的进程空间和包含其中的进程。下面这张图片展示了一个运行中的容器。  
![image](https://user-images.githubusercontent.com/35059921/179346136-a62a80e3-98e0-4704-82e2-ef06116574f2.png)  
### Docker Image 和 Docker Layer (层) 有什么不同？
Image ：一个 Docker Image 是由一系列 Docker 只读层（read-only Layer） 创建出来的。
Layer： 在 Dockerfile 配置文件中完成的一条配置指令，即表示一个 Docker 层（Layer）。
如下 Dockerfile 文件包含 4 条指令，每条指令创建一个层（Layer）。
```
FROM ubuntu:15.04
COPY . /app
RUN make /app
CMD python /app/app.py
```
### 基础命令
#### CMD和ENTRYPOINT
CMD指令为容器提供默认的执行命令；ENTRYPOINT 指令的目的也是为容器指定默认执行的任务。
如果镜像中既没有指定 CMD 也没有指定 ENTRYPOINT 那么在启动容器时会报错。组合使用ENTRYPOINT和CMD, ENTRYPOINT指定默认的运行命令, CMD指定默认的运行参数。
#### Dockerfile常见指令

## K8S
### 组件和原理
#### kube-apiserver
为K8s集群资源操作提供唯一入口，并提供认证、授权、访问控制、API 注册和发现机制。
#### etcd
Raft 核心算法其实就是由个子问题组成：选主（Leader election）、日志复制（Log replication）、安全性（Safety）。这三部分共同实现了 Raft 核心的共识和容错机制。
##### leader election原理
第一阶段：所有节点都是 Follower。  
一个应用 Raft 协议的集群在刚启动（或 Leader 宕机）时，所有节点的状态都是 Follower，初始 Term（任期）为 0。同时启动选举定时器，每个节点的选举定时器超时时间都在 100~500 毫秒之间且并不一致（避免同时发起选举）。  
第二阶段：Follower 转为 Candidate 并发起投票。  
没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），节点启动后在一个选举定时器周期内未收到心跳和投票请求，则状态转为候选者 Candidate 状态，且 Term 自增，并向集群中所有节点发送投票请求并且重置选举定时器。  
第三阶段：投票策略。  
节点收到投票请求后会根据以下情况决定是否接受投票请求（每个 follower 刚成为 Candidate 的时候会将票投给自己）：  
请求节点的 Term 大于自己的 Term，且自己尚未投票给其它节点，则接受请求，把票投给它；  
请求节点的 Term 小于自己的 Term，且自己尚未投票，则拒绝请求，将票投给自己。  
第四阶段：Candidate 转为 Leader。  
一轮选举过后，正常情况下，会有一个 Candidate 收到超过半数节点（N/2 + 1）的投票，它将胜出并升级为 Leader。然后定时发送心跳给其它的节点，其它节点会转为 Follower 并与 Leader 保持同步，到此，本轮选举结束。  

##### Log Replication 原理  
第一阶段：客户端请求提交到 Leader。  
Leader 收到客户端的请求，比如存储数据 5。Leader 在收到请求后，会将它作为日志条目（Entry）写入本地日志中。需要注意的是，此时该 Entry 的状态是未提交（Uncommitted），Leader 并不会更新本地数据，因此它是不可读的。  
第二阶段：Leader 将 Entry 发送到其它 Follower  
Leader 与 Followers 之间保持着心跳联系，随心跳 Leader 将追加的 Entry（AppendEntries）并行地发送给其它的 Follower，并让它们复制这条日志条目，这一过程称为复制（Replicate）  
第三阶段：Leader 等待 Followers 回应。  
Followers 接收到 Leader 发来的复制请求后，有两种可能的回应：  
写入本地日志中，返回 Success；  一致性检查失败，拒绝写入，返回 False，原因和解决办法上面已做了详细说明。  
需要注意的是，此时该 Entry 的状态也是未提交（Uncommitted）。完成上述步骤后，Followers 会向 Leader 发出 Success 的回应，当 Leader 收到大多数 Followers 的回应后，会将第一阶段写入的 Entry 标记为提交状态（Committed），并把这条日志条目应用到它的状态机中。  
第四阶段：Leader 回应客户端。  
第五阶段，Leader 通知 Followers Entry 已提交  
Leader 回应客户端后，将随着下一个心跳通知 Followers，Followers 收到通知后也会将 Entry 标记为提交状态。至此，Raft 集群超过半数节点已经达到一致状态，可以确保强一致性。  
#### kube-scheduler
kube-scheduler 负责监视新创建、未指定运行Node的 Pods，决策出一个让pod运行的节点。  
#### kube-controller-manager  
#### kubelet  
Kubelet是工作节点上的主要服务，定期从kube-apiserver组件接收新的或修改的Pod规范，并确保Pod及其容器在期望规范下运行。同时该组件作为工作节点的监控组件，向kube-apiserver汇报主机的运行状况。  
#### kube-proxy  
Kube-proxy维护节点上的网络规则，实现了Kubernetes Service 概念的一部分 。它的作用是使发往 Service 的流量（通过ClusterIP和端口）负载均衡到正确的后端Pod。  
kube-proxy 监听 API server 中 资源对象的变化情况，包括以下三种：  
1. service  
2. endpoint/endpointslices  
3. node  

目前 Kube-proxy 支持4中代理模式：  
6. userspace  
7. iptables  
8. ipvs  
#### coreDNS
服务发现

### 创建POD的流程
1、首先进行认证（RBAC方式）后获得具体的权限，然后kubectl会调用api创建对象的接口，然后向k8s apiserver发出创建pod的命令。  
2、apiserver收到请求后，先创建一个包含pod创建信息的yaml文件，并将文件信息写入到etcd中。  
3、controller  manager根据配置信息将要创建的资源对象（pod）放到等待队列中。放入队列是一个串行，主要目的还是为了防止应用资源创建的先后顺序和资源调度过程的优先情况。  
4、scheduler（死循环）查看k8s api ，类似于通知机制。根据预选调度和优选调度选出合适的node。  
5、节点上的kubelet进程通过API Server，查看etcd数据库（kubelet通过API Server的WATCH接口监听Pod信息）监听到kube-scheduler产生的Pod绑定事件后获取对应的Pod清单，然后调用本机中的docker  api初始化volume、分配IP、下载image镜像，创建容器并启动服务。  
6、 controller  manager会通过API Server提供的接口实时监控资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将其状态修复到“期望状态”。  

### POD优雅停止
#### POD终止流程
1、Pod 被删除，状态置为 Terminating。  
2、kube-proxy 更新转发规则，将 Pod 从 service 的 endpoint 列表中摘除掉，新的流量不再转发到该 Pod。  
3、如果 Pod 配置了 preStop Hook ，将会执行。  
4、kubelet 对 Pod 中各个 container 发送 SIGTERM 信号以通知容器进程开始优雅停止。  
5、等待容器进程完全停止，如果在 terminationGracePeriodSeconds 内 (默认 30s) 还未完全停止，就发送 SIGKILL 信号强制杀死进程。  
6、所有容器进程终止，清理 Pod 资源。 

在某些极端情况下，Pod 被删除的一小段时间内，仍然可能有新连接被转发过来，因为 kubelet 与 kube-proxy 同时 watch 到 pod 被删除，kubelet 有可能在 kube-proxy 同步完规则前就已经停止容器了，这时可能导致一些新的连接被转发到正在删除的 Pod，而通常情况下，当应用收到 SIGTERM 后都不再接受新连接，只保持存量连接继续处理，所以就可能导致 Pod 删除的瞬间部分请求失败。  
这种情况下，我们也可以利用 preStop 先 sleep 一小下，等待 kube-proxy 完成规则同步再开始停止容器内进程。  

### K8s Informer机制
#### informer机制原理
![image](https://user-images.githubusercontent.com/35059921/179718592-a1d794b1-ec8d-4b13-9de9-e81b7ce812a0.png)
##### Reflector
Reflector的作用，就是通过List&Watch的方式，从apiserver获取到对象以及其状态，然后将其放到一个称为”Delta”的先进先出队列中。  
所谓的List&Watch，就是先调用该API对象的List接口，获取到对象列表，将它们添加到队列中，然后再调用Watch接口，持续监听该API对象的状态变化事件，添加到队列中。
##### Controller
Controller的作用是通过轮询不断从队列中取出Delta元素，根据元素的类型，一方面通过Indexer更新本地的缓存，一方面调用Processor来触发注册到Informer的事件方法。

#### 在整个k8s体系下，是通过哪些手段减少对kube-apiserver的压力？
1、informer机制：
维护本地store(Indexer)从而使得 R 操作直接访问Inxer即可。也即是通过obj-key在indexer中直接取到obj。
ListAndWatch机制，减少与ApiServer的交互，只有在起初通过一次List来全量获取，后续通过watch已增量的方式来更新。
2、sharedInformer机制：
singleton模式：同一个资源只有一个informer实例，多个listener来绑定informer，从而实现一种资源的改动，通过一个informer实例，通知给若干个listener。避免多个listener都与ApiServer打交道。
#### kube-apiserver又是通过哪些手段减少对etcd的压力？
watch cache方面

### 控制器
#### StatefulSet和Deployment
Deployment适合无状态应用的场景。POD之间没有顺序；所有POD共用存储；service都有ClusterI，可以负载均衡。
StatefulSet适合有状态应用的场景。部署、扩展、更新、删除都要有顺序；每个pod都有自己存储，所以都用volumeClaimTemplates，为每个pod都生成一个自己的存储，保存自己的状态；pod名字始终是固定的；service没有ClusterIP，是headlessservice，所以无法负载均衡。

