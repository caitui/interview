## Docker
### cgroup和namespace
**namespace 是用来做资源隔离, cgroup 是用来做资源限制。**
#### cgroup
CGroups用来限定一个进程的资源使用，由Linux 内核支持，可以限制和隔离Linux进程组 (process groups) 所使用的物理资源 ，比如cpu、内存、磁盘。

#### namespace
namespace用来限定一个进程可见的资源，隔离PID(进程ID)、IPC、Network等系统资源。
在每一个namespace内部，每一个用户都拥有一个属于自己的init进程，pid = 1。  
父容器有两个子容器，父容器的命名空间里有两个进程，id分别为3和4, 映射到两个子命名空间后，分别成为其init进程，这样命名空间A和B的用户都认为自己独占整台服务器。
![image](https://user-images.githubusercontent.com/35059921/179401879-145dd2ce-c5bf-4ae8-8803-25fe5d62d8fb.png)  
查看namespace信息：
```
ll /proc/$$/ns
```

### 容器和镜像
![05829b9f34706649fe2c5e683c76b3f](https://user-images.githubusercontent.com/35059921/179188557-5f2bc422-840b-472d-81e9-99402836ebab.png)
#### 镜像
镜像（Image）就是一堆只读层（read-only layer）的统一视角。
#### 容器
容器（container）的定义和镜像（image）几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。
#### 运行态容器
一个运行态容器（running container）被定义为一个可读写的统一文件系统加上隔离的进程空间和包含其中的进程。下面这张图片展示了一个运行中的容器。  
![image](https://user-images.githubusercontent.com/35059921/179346136-a62a80e3-98e0-4704-82e2-ef06116574f2.png)  
### Docker Image 和 Docker Layer (层) 有什么不同？
Image ：一个 Docker Image 是由一系列 Docker 只读层（read-only Layer） 创建出来的。
Layer： 在 Dockerfile 配置文件中完成的一条配置指令，即表示一个 Docker 层（Layer）。
如下 Dockerfile 文件包含 4 条指令，每条指令创建一个层（Layer）。
```
FROM ubuntu:15.04
COPY . /app
RUN make /app
CMD python /app/app.py
```
### 基础命令
#### CMD和ENTRYPOINT
CMD指令为容器提供默认的执行命令；ENTRYPOINT 指令的目的也是为容器指定默认执行的任务。
如果镜像中既没有指定 CMD 也没有指定 ENTRYPOINT 那么在启动容器时会报错。组合使用ENTRYPOINT和CMD, ENTRYPOINT指定默认的运行命令, CMD指定默认的运行参数。
#### Dockerfile常见指令

## K8S
### 组件和原理
#### kube-apiserver
为K8s集群资源操作提供唯一入口，并提供认证、授权、访问控制、API 注册和发现机制。
#### etcd
Raft 核心算法其实就是由个子问题组成：选主（Leader election）、日志复制（Log replication）、安全性（Safety）。这三部分共同实现了 Raft 核心的共识和容错机制。
##### leader election原理
第一阶段：所有节点都是 Follower。  
一个应用 Raft 协议的集群在刚启动（或 Leader 宕机）时，所有节点的状态都是 Follower，初始 Term（任期）为 0。同时启动选举定时器，每个节点的选举定时器超时时间都在 100~500 毫秒之间且并不一致（避免同时发起选举）。  
第二阶段：Follower 转为 Candidate 并发起投票。  
没有 Leader，Followers 无法与 Leader 保持心跳（Heart Beat），节点启动后在一个选举定时器周期内未收到心跳和投票请求，则状态转为候选者 Candidate 状态，且 Term 自增，并向集群中所有节点发送投票请求并且重置选举定时器。  
第三阶段：投票策略。  
节点收到投票请求后会根据以下情况决定是否接受投票请求（每个 follower 刚成为 Candidate 的时候会将票投给自己）：  
请求节点的 Term 大于自己的 Term，且自己尚未投票给其它节点，则接受请求，把票投给它；  
请求节点的 Term 小于自己的 Term，且自己尚未投票，则拒绝请求，将票投给自己。  
第四阶段：Candidate 转为 Leader。  
一轮选举过后，正常情况下，会有一个 Candidate 收到超过半数节点（N/2 + 1）的投票，它将胜出并升级为 Leader。然后定时发送心跳给其它的节点，其它节点会转为 Follower 并与 Leader 保持同步，到此，本轮选举结束。  

##### Log Replication 原理  
第一阶段：客户端请求提交到 Leader。  
Leader 收到客户端的请求，比如存储数据 5。Leader 在收到请求后，会将它作为日志条目（Entry）写入本地日志中。需要注意的是，此时该 Entry 的状态是未提交（Uncommitted），Leader 并不会更新本地数据，因此它是不可读的。  
第二阶段：Leader 将 Entry 发送到其它 Follower  
Leader 与 Followers 之间保持着心跳联系，随心跳 Leader 将追加的 Entry（AppendEntries）并行地发送给其它的 Follower，并让它们复制这条日志条目，这一过程称为复制（Replicate）  
第三阶段：Leader 等待 Followers 回应。  
Followers 接收到 Leader 发来的复制请求后，有两种可能的回应：  
写入本地日志中，返回 Success；  一致性检查失败，拒绝写入，返回 False，原因和解决办法上面已做了详细说明。  
需要注意的是，此时该 Entry 的状态也是未提交（Uncommitted）。完成上述步骤后，Followers 会向 Leader 发出 Success 的回应，当 Leader 收到大多数 Followers 的回应后，会将第一阶段写入的 Entry 标记为提交状态（Committed），并把这条日志条目应用到它的状态机中。  
第四阶段：Leader 回应客户端。  
第五阶段，Leader 通知 Followers Entry 已提交  
Leader 回应客户端后，将随着下一个心跳通知 Followers，Followers 收到通知后也会将 Entry 标记为提交状态。至此，Raft 集群超过半数节点已经达到一致状态，可以确保强一致性。  
#### kube-scheduler
kube-scheduler 负责监视新创建、未指定运行Node的 Pods，决策出一个让pod运行的节点。  
#### kube-controller-manager  
#### kubelet  
Kubelet是工作节点上的主要服务，定期从kube-apiserver组件接收新的或修改的Pod规范，并确保Pod及其容器在期望规范下运行。同时该组件作为工作节点的监控组件，向kube-apiserver汇报主机的运行状况。  
#### kube-proxy  
Kube-proxy维护节点上的网络规则，实现了Kubernetes Service 概念的一部分 。它的作用是使发往 Service 的流量（通过ClusterIP和端口）负载均衡到正确的后端Pod。  
kube-proxy 监听 API server 中 资源对象的变化情况，包括以下三种：  
1. service  
2. endpoint/endpointslices  
3. node  
目前 Kube-proxy 支持4中代理模式：  
1. userspace  
2. iptables  
3. ipvs  
#### coreDNS
服务发现

### 创建POD的流程
1、首先进行认证（RBAC方式 或者 key方式进行认证 ）后获得具体的权限，然后kubectl会调用master  api创建对象的接口，然后向k8s  apiserver发出创建pod的命令  
2、apiserver收到请求后，先创建一个包含pod创建信息的yaml文件，并将文件信息写入到etcd中  
3、controller  manager根据配置信息将要创建的资源对象（pod）放到等待队列中。放入队列是一个串行，主要目的还是为了防止应用资源创建的先后顺序和资源调度过程的优先情况。  
4、 scheduler （死循环）查看k8s api ，类似于通知机制。根据预选调度和优选调度选出合适的node。  
5、节点上的kubelet进程通过API Server，查看etcd数据库（kubelet通过API Server的WATCH接口监听Pod信息）监听到kube-scheduler产生的Pod绑定事件后获取对应的Pod清单，然后调用本机中的docker  api初始化volume、分配IP、下载image镜像，创建容器并启动服务。
6、 controller  manager会通过API Server提供的接口实时监控资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将其状态修复到“期望状态”。

